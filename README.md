# ğŸ§  ChatGPT Local com Docker + Ollama

![Docker](https://img.shields.io/badge/Docker-Container-blue) ![Python](https://img.shields.io/badge/Python-3.10-green) ![Flask](https://img.shields.io/badge/Flask-Framework-orange)

Uma aplicaÃ§Ã£o local de **ChatGPT** usando **Flask** e **Ollama**, totalmente containerizada. FaÃ§a perguntas na interface web e receba respostas usando o modelo **Llama2**.

---

## âœ¨ Funcionalidades

* ğŸ–¥ Interface web simples para digitar perguntas
* ğŸ¤– Respostas via modelo **Llama2** dentro do container Ollama
* ğŸ“¦ Totalmente containerizada para fÃ¡cil execuÃ§Ã£o em qualquer mÃ¡quina com Docker
* ğŸ”„ AtualizaÃ§Ã£o simples dos containers e do modelo

---

## âš ï¸ PrÃ©-requisitos

* [Docker](https://www.docker.com/)
* [Docker Compose](https://docs.docker.com/compose/)
* ConexÃ£o Ã  internet (para baixar Ollama e o modelo Llama2)
* **ğŸ’¾ Pelo menos 6â€¯GB de RAM disponÃ­veis** para o container Ollama

> Sem memÃ³ria suficiente, o modelo **Llama2** nÃ£o serÃ¡ carregado e retornarÃ¡ erro.

---

## ğŸš€ InstalaÃ§Ã£o e Uso

1. **Clonar o repositÃ³rio**

```bash
git clone https://github.com/josehenriqueprogramador/chatgpt_local.git
cd chatgpt_local
```

2. **Subir os containers**

```bash
docker-compose up -d --build
```

Isso criarÃ¡ dois containers:

* `chatgpt_local` â†’ aplicaÃ§Ã£o Flask
* `ollama` â†’ servidor Ollama

3. **Baixar o modelo Llama2 dentro do container Ollama**

```bash
docker exec -it ollama /bin/sh
ollama pull llama2
exit
```

> Este passo Ã© obrigatÃ³rio para que o ChatGPT local funcione.

4. **Acessar a aplicaÃ§Ã£o**

Abra no navegador:

```
http://localhost:5000
```

Digite sua pergunta e veja a resposta do ChatGPT.

---

## ğŸ³ Usando a imagem do Docker Hub

A imagem do ChatGPT local estÃ¡ disponÃ­vel no Docker Hub:

```bash
docker pull josehenriquejardim/chatgpt_local:latest
docker run -p 5000:5000 josehenriquejardim/chatgpt_local:latest
```

---

## ğŸ“‚ Estrutura do Projeto

```
chatgpt_local/
â”‚
â”œâ”€ app.py
â”œâ”€ Dockerfile
â”œâ”€ docker-compose.yml
â”œâ”€ templates/
â”‚   â””â”€ index.html
â”œâ”€ static/
â”‚   â””â”€ style.css
```

---

## ğŸ’¡ Dicas e ObservaÃ§Ãµes

* Certifique-se de que o container Ollama estÃ¡ rodando e o modelo Llama2 foi baixado.
* Ajuste a variÃ¡vel `OLLAMA_HOST` em `app.py` caso queira conectar a outro host ou porta.
* **MemÃ³ria mÃ­nima de 6â€¯GB Ã© necessÃ¡ria** para rodar Llama2 sem erros.
* VocÃª pode forÃ§ar atualizaÃ§Ã£o do modelo usando:

```bash
docker exec -it ollama /bin/sh
ollama pull llama2
```

* Para parar os containers:

```bash
docker-compose down
```

---
